# -*- coding: utf-8 -*-
"""retrieval-comparison-L1-norm-bill-evans.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kWkAXj8EqRNSLnhV8Gvgv3I_reLhaDyU

# Evaluating Timbral Extraction Methods on Real Data üëÄüéµ

In this Jupyter notebook, we will explore the hypothesis that the embedding method with the most structure in the TSNE (t-Distributed Stochastic Neighbor Embedding) space will yield the best similarity results, based on previously obtained TSNE results for various timbral extraction methods. üìäüîç

## Introduction

In this investigation, we will compare four different methods for feature extraction on audio data: MFCC (Mel-Frequency Cepstral Coefficients), Spectrogram, raw audio (used as a baseline), and a deep learning feature extraction method called "musicnn." üé∂üî¨

## Methods

1. **MFCC**: Mel-Frequency Cepstral Coefficients are commonly used features for audio processing. They capture the spectral characteristics of audio signals, providing information about the frequency content over time.

2. **Spectrogram**: Spectrograms provide a visual representation of the frequencies present in an audio signal over time. They are created by performing a Fourier Transform on small overlapping windows of the audio signal.

3. **Raw Audio**: The raw audio waveform itself can be used as a baseline feature representation. It captures the amplitude values of the audio signal directly, without any additional processing.

4. **musicnn**: This deep learning feature extraction method is described in the paper titled "musicnn: Pretrained Convolutional Neural Networks for Music Audio Tagging" by Jordi Pons et al. [Link to paper](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&cad=rja&uact=8&ved=2ahUKEwjqnu3Vi_H-AhXdFlkFHYXkBtgQFnoECBAQAQ&url=https%3A%2F%2Farxiv.org%2Fabs%2F1909.06654&usg=AOvVaw3JXWyJLxhkjoyz7lLg8mSO).

## Experimental Design

1. We will use a dataset of audio samples and cooresponding synthesizer paramters that produced those sounds.

2. Each audio sample will be processed using the four different feature extraction methods: MFCC, Spectrogram, raw audio, and musicnn.

3. We will pass in a target sound, obtain its timbre, and find the preset in the dataset with the nearest timbre based on cosine similarity.

4. Finally, we will evaluate and compare each timbral extraction method empirically to determine which method yields the closest sounding instrument.

## Expected Outcome

Based on our hypothesis, we expect that the embedding method with the most structure in the TSNE space will yield the best similarity results. By comparing the performance of the four feature extraction methods, we can gain insights into the effectiveness of each method for audio similarity analysis.

Let's proceed with the implementation and analysis to validate our hypothesis! üìäüî¨üéµ

## Imports
"""

import sys; sys.path.append('../')
import librosa
import pandas as pd
import os
import numpy as np
import torch
import dawdreamer
import matplotlib.pyplot as plt
from src.utils import *
from IPython.display import Audio
from musicnn.extractor import extractor
from scipy.io.wavfile import write as wav_write

"""## Initialize the DawDreamer Synthesizer"""

# define the preset path location
preset_path = '/Users/malek8/Library/Application Support/ToguAudioLine/TAL-U-No-LX/presets'

# initialize the dawdreamer engine
engine = dawdreamer.RenderEngine(sample_rate=44100, block_size=128)

# load the plugin
plugin = load_plugin_with_dawdreamer("/Library/Audio/Plug-Ins/VST3/TAL-U-NO-LX-V2.vst3","TAL-Uno",engine)

"""## Load a Target Sound Snippet"""

# specify the path to the dataset
dataset_path = 'target-dataset'
sample_name = 'bill-evans-piano.wav'
target_path = f'{dataset_path}{os.sep}{sample_name}'

# Load the target audio
target_audio, target_sr = librosa.load(target_path, sr=44100)

# play the target audio
Audio(target_audio, rate=target_sr)

"""### Find the Fundamental Frequency of the Target Audio"""

ff = get_fundamental_frequency(target_audio, target_sr)

"""### Find which of C2, C3, C4 the FF is Closest to"""

closest_note = get_closest_note_from_ff(ff[1])
print(f'Closest note: {closest_note}')

"""## Load the Preset Dataset File"""

# specify the dataset path
dataset_path = '../dataset/preset_dataset_musicnn.pt'

# load and convert the dataset to a dataframe
dataset = pd.DataFrame(torch.load(dataset_path))

"""## Evaluate Raw Audio as Baseline

### Find Closest Preset to Target Based on Euclidean Distance
"""

# define the comparison vectors
comparison_audio_set = np.stack(dataset['raw_audio'])
print(f'Comparison audio set shape: {comparison_audio_set.shape}')

comparison_audio_set = np.stack([x[closest_note] for x in dataset['raw_audio']])
print(f'Comparison audio set shape: {comparison_audio_set.shape}')
print('Target audio shape: ', target_audio.shape)

# adapt the comparison audio set to the target audio if their lengths are not equal
if comparison_audio_set.shape[1] < target_audio.shape[0]:
    comparison_audio_set = np.pad(comparison_audio_set, ((0, 0), (0, target_audio.shape[0] - comparison_audio_set.shape[1])), 'constant', constant_values=0)
elif comparison_audio_set.shape[1] > target_audio.shape[0]:
    comparison_audio_set = comparison_audio_set[:, :target_audio.shape[0]]
print(f'Comparison audio set shape: {comparison_audio_set.shape}')
print(f'Target audio shape: {target_audio.shape}')

# find where the euclidean distance is the smallest
distances = np.linalg.norm(comparison_audio_set - target_audio, axis=1)
print(f'Distances shape: {distances.shape}')

# find the index of the smallest distance
raw_idx = np.argmin(distances)
print(f'Index: {raw_idx}')

# plot the distances from smallest to largest
plt.plot(np.sort(distances))
plt.show()

# get the preset name
raw_audio_preset = dataset.iloc[raw_idx]['preset_names']
print(f'Raw audio preset: {raw_audio_preset}')

# load the preset to the plugin
# preset_file = f'{preset_path}{os.sep}{raw_audio_preset}.pjunoxl'
# json_file_location = make_json_parameter_mapping(plugin,preset_file)
# loaded_preset_synth = load_xml_preset(plugin,json_file_location)

# play the audio assoicated with the preset
Audio(dataset.iloc[raw_idx]['raw_audio'][closest_note], rate=44100)

# play the target audio
Audio(target_audio, rate=44100)

"""## Evaluate Spectrogram Timbral Feature

### Evaluate Euclidean Distance
"""

test = np.stack([x[closest_note] for x in dataset['raw_audio']])
print(f'Test shape: {test.shape}')

test = wav2spec(comparison_audio_set[0,:],target_sr)
print(f'Test shape: {test.shape}')

# Define the comparison vectors
comparison_spec_set = np.stack([wav2spec(x, 44100) for x in comparison_audio_set])
print(f'Comparison spec set shape: {comparison_spec_set.shape}')

# flatten the spectrogram set except for the first dimension
comparison_spec_set = comparison_spec_set.reshape(comparison_spec_set.shape[0], -1)
print(f'Comparison spec set shape: {comparison_spec_set.shape}')

# find where the euclidean distance is the smallest
distances = np.linalg.norm(comparison_spec_set - wav2spec(target_audio, 44100).reshape(1, -1), axis=1)
print(f'Distances shape: {distances.shape}')

# find the index of the smallest distance
spec_idx = np.argmin(distances)
print(f'Index: {spec_idx}')

# plot the distances from smallest to largest
plt.plot(np.sort(distances))
plt.show()

# get the preset name
spec_preset = dataset.iloc[spec_idx]['preset_names']
print(f'Spec preset: {spec_preset}')

# compare the spectrogram of the target audio with the spectrogram of the preset using matplotlib
plt.figure(figsize=(10,5))
plt.subplot(1,2, 1)
plt.title('Target audio')
plt.imshow(wav2spec(target_audio, 44100),aspect='auto')
plt.subplot(1, 2, 2)
plt.title('Preset audio')
plt.imshow(wav2spec(dataset.iloc[spec_idx]['raw_audio'][closest_note].numpy()[:target_audio.shape[0]], 44100),aspect='auto')
plt.tight_layout()
plt.show()

# play the audio assoicated with the preset
Audio(dataset.iloc[spec_idx]['raw_audio']['C4'], rate=44100)

# play the target audio
Audio(target_audio, rate=44100)

"""## Evaluate MFCC Timbral Feature

### Evaluate Euclidean Distance
"""

# define the comparison vectors
comparison_mfcc_set = np.stack([librosa.feature.mfcc(y=x, sr=44100) for x in comparison_audio_set])
print(f'Comparison mfcc set shape: {comparison_mfcc_set.shape}')

# convert the comparison set to a 2D array
comparison_mfcc_set = comparison_mfcc_set.reshape(comparison_mfcc_set.shape[0], -1)
print(f'Comparison mfcc set shape: {comparison_mfcc_set.shape}')

# find where the euclidean distance is the smallest
distances = np.linalg.norm(comparison_mfcc_set - librosa.feature.mfcc(y=target_audio, sr=44100).reshape(1, -1), axis=1)
print(f'Distances shape: {distances.shape}')

# find the index of the smallest distance
mfcc_idx = np.argmin(distances)
print(f'Index: {mfcc_idx}')

# plot the distances from smallest to largest
plt.plot(np.sort(distances))
plt.show()

# get the preset name
mfcc_preset = dataset.iloc[mfcc_idx]['preset_names']
print(f'MFCC preset: {mfcc_preset}')

# find the top 10 closest presets
top_10 = np.argsort(distances)[:10]

# list the preset names of the top 10 closest presets
print(f'Top 10 closest presets: {dataset.iloc[top_10]["preset_names"].values}')

# visualize the MFCCs of the target audio and the preset
plt.figure(figsize=(10,5))
plt.subplot(1,2, 1)
plt.title('Target audio')
plt.imshow(librosa.feature.mfcc(y=target_audio, sr=44100),aspect='auto')
plt.subplot(1, 2, 2)
plt.title('Preset audio')
plt.imshow(librosa.feature.mfcc(y=dataset.iloc[mfcc_idx]['raw_audio'][closest_note].numpy()[:target_audio.shape[0]], sr=44100),aspect='auto')
# add a colorbar
plt.colorbar()
plt.tight_layout()

# visualize the spectrogram of the target audio and the preset
plt.figure(figsize=(10,5))
plt.subplot(1,2, 1)
plt.title('Target audio')
plt.imshow(wav2spec(target_audio, 44100),aspect='auto')
plt.subplot(1, 2, 2)
plt.title('Preset audio')
plt.imshow(wav2spec(dataset.iloc[mfcc_idx]['raw_audio'][closest_note].numpy()[:target_audio.shape[0]], 44100),aspect='auto')
# add a colorbar
plt.colorbar()
plt.tight_layout()
plt.show()

# play the audio assoicated with the preset
Audio(dataset.iloc[mfcc_idx]['raw_audio'][closest_note], rate=44100)

# play the target audio
Audio(target_audio, rate=44100)

"""## Evalute musicnn Timbral Feature

### Evaluate Euclidean Distance
"""

# define the comparison vectors
comparison_musicnn_set = np.stack(dataset['musicnn_features'])
print(f'Comparison musicnn set shape: {comparison_musicnn_set.shape}')

# process the dataset so that it uses the closest note
comparison_musicnn_set = np.stack([x[closest_note] for x in comparison_musicnn_set])
print(f'Comparison musicnn set shape: {comparison_musicnn_set.shape}')

# pad the target audio so that it is 3 seconds long
target_audio = np.pad(target_audio, (0, 132300 - target_audio.shape[0]), 'constant')
print(f'Target audio shape: {target_audio.shape}')

# save the target audio as a temporary wav file
temp_file_name = 'temp_audio.wav'
wav_write(temp_file_name, 44100, target_audio)

# process the target audio through musicnn
taggram, tag, features = extractor(temp_file_name, model='MTT_musicnn', extract_features=True)
timbral_features = features['timbral'][0,:]
print(f'Features shape: {timbral_features.shape}')
print(f'Taggram shape: {taggram.shape}')

# visualize the taggram
in_length = 3 # seconds -- by default, the model takes inputs of 3 seconds with no overlap

# depict taggram
plt.rcParams["figure.figsize"] = (10,8)
fontsize=12
fig, ax = plt.subplots()
ax.imshow(taggram.T, interpolation=None, aspect="auto")

# title
ax.title.set_text('Taggram')
ax.title.set_fontsize(fontsize)

# x-axis title
ax.set_xlabel('(seconds)', fontsize=fontsize)

# y-axis
y_pos = np.arange(len(tag))
ax.set_yticks(y_pos)
ax.set_yticklabels(tag, fontsize=fontsize-1)

# x-axis
x_pos = np.arange(taggram.shape[0])
x_label = np.arange(in_length/2, in_length*taggram.shape[0], 3)
ax.set_xticks(x_pos)
ax.set_xticklabels(x_label, fontsize=fontsize)

plt.show()

# find where the euclidean distance is the smallest
distances = np.linalg.norm(comparison_musicnn_set - timbral_features.reshape(-1), axis=1)
print(f'Distances shape: {distances.shape}')

# find the index of the smallest distance
musicnn_idx = np.argmin(distances)
print(f'Index: {musicnn_idx}')

# find the top 10 closest presets
top_10 = np.argsort(distances)[:10]

# list the preset names of the top 10 closest presets
print(f'Top 10 closest presets: {dataset.iloc[top_10]["preset_names"].values}')

# plot the distances from smallest to largest
plt.plot(np.sort(distances))
plt.show()

# get the preset name
musicnn_preset = dataset.iloc[musicnn_idx]['preset_names']
print(f'Musicnn preset: {musicnn_preset}')

# visualize the spectrogram of the target audio and the preset
plt.figure(figsize=(10,5))
plt.subplot(1,2, 1)
plt.title('Target audio')
plt.imshow(wav2spec(target_audio, 44100),aspect='auto')
plt.subplot(1, 2, 2)
plt.title('Preset audio')
plt.imshow(wav2spec(dataset.iloc[musicnn_idx]['raw_audio'][closest_note].numpy()[:target_audio.shape[0]], 44100),aspect='auto')
# add a colorbar
plt.colorbar()
plt.tight_layout()

# play the audio assoicated with the preset
Audio(dataset.iloc[musicnn_idx]['raw_audio'][closest_note], rate=44100)

# play the target audio
Audio(target_audio, rate=44100)